# 🚀 云端+本地爬虫架构部署指南

## 📋 架构说明

```
┌─────────────────────────────────────────────────────────┐
│          云端服务器（Railway/Render - 24小时运行）        │
│                                                           │
│  ✅ AI简历分析（DeepSeek）                                │
│  ✅ 用户界面（Web）                                       │
│  ✅ 岗位数据API（接收爬虫推送）                           │
│  ✅ 所有人都可以访问                                      │
└─────────────────────────────────────────────────────────┘
                       ↑
                       │ HTTPS推送岗位数据
                       │
┌─────────────────────────────────────────────────────────┐
│          您的本地电脑（OpenClaw爬虫 - 定时运行）          │
│                                                           │
│  ✅ OpenClaw控制Chrome浏览器                              │
│  ✅ 定时爬取Boss直聘岗位                                  │
│  ✅ 清洗数据并推送到云端                                  │
│  ✅ 只有您需要运行                                        │
└─────────────────────────────────────────────────────────┘
```

## 🎯 优势

1. **云端24小时在线** - 用户随时可以访问
2. **真实岗位数据** - 您的本地爬虫提供真实Boss直聘数据
3. **安全合规** - 使用您自己的浏览器和登录态
4. **成本低** - 云端只需运行AI分析，不需要浏览器
5. **可扩展** - 可以多台电脑同时爬取，提高数据覆盖

---

## 📦 第一步：云端部署

### 1.1 选择云平台

推荐使用 **Railway** 或 **Render**（免费额度足够）

### 1.2 部署到 Railway

```bash
# 1. 安装 Railway CLI
npm install -g @railway/cli

# 2. 登录
railway login

# 3. 初始化项目
railway init

# 4. 部署
railway up
```

### 1.3 配置环境变量（云端）

在 Railway/Render 控制台添加：

```env
# 必填
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# 爬虫API密钥（生成一个随机密钥）
CRAWLER_API_KEY=your-secret-key-change-this

# 岗位数据源（云端使用cloud模式）
JOB_DATA_PROVIDER=cloud

# 端口
PORT=8000
```

**生成安全的API密钥：**
```bash
python -c "import secrets; print(secrets.token_urlsafe(32))"
```

### 1.4 获取云端URL

部署成功后，您会得到一个URL，例如：
```
https://your-app.railway.app
```

记下这个URL，后面配置爬虫时需要用到。

---

## 🤖 第二步：本地爬虫配置

### 2.1 安装依赖

```bash
# 安装Python依赖
pip install -r requirements.txt

# 安装schedule（定时任务）
pip install schedule
```

### 2.2 配置爬虫

1. 复制配置文件：
```bash
copy crawler.env.example crawler.env
```

2. 编辑 `crawler.env`：
```env
# 云端API地址（替换为您的实际地址）
CLOUD_API_URL=https://your-app.railway.app

# 爬虫API密钥（与云端保持一致）
CRAWLER_API_KEY=your-secret-key-change-this

# 爬取间隔（小时）
CRAWL_INTERVAL_HOURS=6

# OpenClaw配置
OPENCLAW_BROWSER_PROFILE=chrome
OPENCLAW_JOB_SITES=boss
```

### 2.3 配置OpenClaw

```bash
# 1. 安装OpenClaw（如果还没安装）
# 访问 https://github.com/getcursor/openclaw

# 2. 启动OpenClaw浏览器
openclaw browser start

# 3. 在Chrome中打开Boss直聘并登录
# https://www.zhipin.com

# 4. 点击OpenClaw扩展图标，点击"Attach"
```

### 2.4 启动爬虫服务

```bash
# 方式1：使用启动脚本
启动爬虫服务.bat

# 方式2：直接运行
python openclaw_crawler_service.py
```

---

## 🔄 第三步：验证整个流程

### 3.1 验证云端服务

访问：`https://your-app.railway.app/api/health`

应该返回：
```json
{
  "status": "ok",
  "message": "AI求职助手运行正常"
}
```

### 3.2 验证爬虫推送

启动爬虫后，查看控制台输出：
```
🚀 开始定时爬取任务 - 2025-02-10 12:00:00
🔍 开始爬取：['Python', '后端开发'] @ 北京
✅ 爬取成功：10 个岗位
✅ 推送成功：10 个岗位
...
✅ 本次任务完成：共爬取并推送 480 个岗位
```

### 3.3 验证云端接收

访问：`https://your-app.railway.app/api/crawler/status`

应该返回：
```json
{
  "status": "ok",
  "total": 480
}
```

### 3.4 验证用户访问

访问：`https://your-app.railway.app/app`

用户应该能够：
1. 上传简历
2. AI分析
3. 看到真实的Boss直聘岗位（来自您的爬虫）

---

## 📊 第四步：监控和维护

### 4.1 爬虫监控

爬虫会每6小时自动运行一次，您可以：

1. **查看控制台输出** - 实时查看爬取进度
2. **检查云端状态** - 访问 `/api/crawler/status`
3. **调整爬取频率** - 修改 `CRAWL_INTERVAL_HOURS`

### 4.2 云端监控

访问 Railway/Render 控制台查看：
- 服务运行状态
- 日志输出
- 资源使用情况

### 4.3 数据更新策略

```
爬虫运行频率：每6小时
每次爬取岗位：约500个
云端缓存上限：5000个（自动清理旧数据）
数据新鲜度：最多6小时
```

---

## 🛠️ 常见问题

### Q1: 爬虫推送失败怎么办？

**检查清单：**
1. 云端URL是否正确？
2. API密钥是否一致？
3. 网络是否正常？
4. 云端服务是否在线？

**解决方法：**
```bash
# 测试云端连接
curl https://your-app.railway.app/api/health

# 测试推送接口
curl -X POST https://your-app.railway.app/api/crawler/upload \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{"jobs":[],"timestamp":"2025-02-10T12:00:00","source":"test"}'
```

### Q2: OpenClaw连接失败？

参考之前的 OpenClaw 配置文档：
- 确保Chrome已打开Boss直聘
- 确保OpenClaw扩展已Attach
- 尝试重新Attach

### Q3: 云端数据为空？

**原因：**
- 爬虫还没开始运行
- 爬虫推送失败
- 云端服务重启（内存数据丢失）

**解决：**
1. 启动本地爬虫
2. 等待首次爬取完成（约10-20分钟）
3. 检查 `/api/crawler/status`

### Q4: 如何扩展爬取范围？

编辑 `openclaw_crawler_service.py`：

```python
# 添加更多关键词
self.hot_keywords = [
    ["Python", "后端开发"],
    ["Java", "Spring Boot"],
    # 添加您想要的关键词...
]

# 添加更多城市
self.hot_cities = ["北京", "上海", "深圳", "杭州", "广州", "成都"]
```

### Q5: 如何持久化数据？

当前使用内存存储，云端重启会丢失数据。

**升级方案：**
1. 使用 Redis（推荐）
2. 使用 PostgreSQL
3. 使用 MongoDB

---

## 💰 成本估算

### 云端（Railway免费版）
- ✅ 每月500小时免费
- ✅ 足够24小时运行
- ✅ 超出后约$5/月

### 本地爬虫
- ✅ 完全免费
- ✅ 只需保持电脑开机
- ✅ 可以设置定时开关机

### DeepSeek API
- ✅ 非常便宜
- ✅ 约$0.001/次分析
- ✅ 1000次分析约$1

**总成本：约$5-10/月**

---

## 🚀 进阶优化

### 1. 多台爬虫

可以在多台电脑上运行爬虫，提高数据覆盖：

```env
# 电脑A：爬取北上广深
CRAWL_CITIES=北京,上海,广州,深圳

# 电脑B：爬取杭州成都
CRAWL_CITIES=杭州,成都
```

### 2. 智能调度

根据时间段调整爬取频率：

```python
# 工作日白天：每2小时
# 工作日晚上：每6小时
# 周末：每12小时
```

### 3. 数据分析

添加数据分析功能：
- 岗位趋势分析
- 薪资范围统计
- 热门技能排行

---

## 📞 获取帮助

- 📖 查看完整文档：`docs/完整使用指南.md`
- 🐛 报告问题：GitHub Issues
- 💬 技术交流：加入社区

---

**祝您部署顺利！🎉**

