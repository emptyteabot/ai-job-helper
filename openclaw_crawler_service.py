"""
OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œï¼Œå®šæ—¶çˆ¬å–Bossç›´è˜å²—ä½å¹¶æ¨é€åˆ°äº‘ç«¯
"""

import os
import time
import json
import requests
from datetime import datetime
from typing import List, Dict, Any
import schedule
from app.services.job_providers.openclaw_browser_provider import OpenClawBrowserProvider
from app.services.job_providers.base import JobSearchParams

class OpenClawCrawlerService:
    """OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œ"""
    
    def __init__(self, cloud_api_url: str, api_key: str):
        """
        åˆå§‹åŒ–çˆ¬è™«æœåŠ¡
        
        Args:
            cloud_api_url: äº‘ç«¯APIåœ°å€ï¼Œå¦‚ https://your-app.railway.app
            api_key: APIå¯†é’¥ï¼Œç”¨äºè®¤è¯
        """
        self.cloud_api_url = cloud_api_url.rstrip('/')
        self.api_key = api_key
        self.openclaw = OpenClawBrowserProvider()
        
        # é¢„å®šä¹‰çš„çƒ­é—¨æœç´¢å…³é”®è¯
        self.hot_keywords = [
            ["Python", "åç«¯å¼€å‘"],
            ["Java", "Spring Boot"],
            ["å‰ç«¯", "React", "Vue"],
            ["ç®—æ³•å·¥ç¨‹å¸ˆ", "æœºå™¨å­¦ä¹ "],
            ["æ•°æ®åˆ†æ", "SQL"],
            ["äº§å“ç»ç†"],
            ["æµ‹è¯•å·¥ç¨‹å¸ˆ", "è‡ªåŠ¨åŒ–"],
            ["è¿ç»´", "DevOps", "Kubernetes"],
        ]
        
        self.hot_cities = ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "æ­å·", "å¹¿å·", "æˆéƒ½"]
    
    def crawl_jobs(self, keywords: List[str], location: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        çˆ¬å–å²—ä½æ•°æ®
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            location: åŸå¸‚
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            å²—ä½åˆ—è¡¨
        """
        print(f"ğŸ” å¼€å§‹çˆ¬å–ï¼š{keywords} @ {location}")
        
        try:
            params = JobSearchParams(
                keywords=keywords,
                location=location,
                limit=limit
            )
            
            jobs = self.openclaw.search_jobs(params)
            print(f"âœ… çˆ¬å–æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}")
            return []
    
    def push_to_cloud(self, jobs: List[Dict[str, Any]]) -> bool:
        """
        æ¨é€å²—ä½æ•°æ®åˆ°äº‘ç«¯
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not jobs:
            return False
        
        try:
            url = f"{self.cloud_api_url}/api/crawler/upload"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "jobs": jobs,
                "timestamp": datetime.now().isoformat(),
                "source": "openclaw_local"
            }
            
            response = requests.post(url, json=data, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æ¨é€æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥ï¼š{response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")
            return False
    
    def crawl_and_push_all(self):
        """çˆ¬å–æ‰€æœ‰çƒ­é—¨å…³é”®è¯å¹¶æ¨é€"""
        print("\n" + "="*60)
        print(f"ğŸš€ å¼€å§‹å®šæ—¶çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        total_jobs = 0
        
        for keywords in self.hot_keywords:
            for city in self.hot_cities:
                # çˆ¬å–å²—ä½
                jobs = self.crawl_jobs(keywords, city, limit=10)
                
                if jobs:
                    # æ¨é€åˆ°äº‘ç«¯
                    if self.push_to_cloud(jobs):
                        total_jobs += len(jobs)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(5)
        
        print("\n" + "="*60)
        print(f"âœ… æœ¬æ¬¡ä»»åŠ¡å®Œæˆï¼šå…±çˆ¬å–å¹¶æ¨é€ {total_jobs} ä¸ªå²—ä½")
        print("="*60 + "\n")
    
    def start_scheduled_crawling(self, interval_hours: int = 6):
        """
        å¯åŠ¨å®šæ—¶çˆ¬å–
        
        Args:
            interval_hours: çˆ¬å–é—´éš”ï¼ˆå°æ—¶ï¼‰
        """
        print("\n" + "ğŸ¤–"*30)
        print("OpenClawçˆ¬è™«æœåŠ¡å¯åŠ¨")
        print("ğŸ¤–"*30)
        print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯ï¼š")
        print(f"  - äº‘ç«¯API: {self.cloud_api_url}")
        print(f"  - çˆ¬å–é—´éš”: æ¯ {interval_hours} å°æ—¶")
        print(f"  - å…³é”®è¯æ•°: {len(self.hot_keywords)}")
        print(f"  - åŸå¸‚æ•°: {len(self.hot_cities)}")
        print(f"\nâš ï¸ è¯·ç¡®ä¿ï¼š")
        print(f"  1. Chromeå·²æ‰“å¼€Bossç›´è˜å¹¶ç™»å½•")
        print(f"  2. OpenClawæ‰©å±•å·²Attachåˆ°æ ‡ç­¾é¡µ")
        print(f"  3. ä¿æŒæµè§ˆå™¨çª—å£ä¸è¦å…³é—­")
        print(f"\nğŸ”„ é¦–æ¬¡çˆ¬å–å°†åœ¨å¯åŠ¨åç«‹å³å¼€å§‹...\n")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        self.crawl_and_push_all()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_hours).hours.do(self.crawl_and_push_all)
        
        print(f"â° ä¸‹æ¬¡çˆ¬å–æ—¶é—´ï¼š{interval_hours} å°æ—¶å")
        print(f"ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    # é…ç½®
    CLOUD_API_URL = os.getenv("CLOUD_API_URL", "https://your-app.railway.app")
    API_KEY = os.getenv("CRAWLER_API_KEY", "your-secret-key")
    INTERVAL_HOURS = int(os.getenv("CRAWL_INTERVAL_HOURS", "6"))
    
    # å¯åŠ¨çˆ¬è™«æœåŠ¡
    crawler = OpenClawCrawlerService(
        cloud_api_url=CLOUD_API_URL,
        api_key=API_KEY
    )
    
    try:
        crawler.start_scheduled_crawling(interval_hours=INTERVAL_HOURS)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ çˆ¬è™«æœåŠ¡å·²åœæ­¢")

OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œï¼Œå®šæ—¶çˆ¬å–Bossç›´è˜å²—ä½å¹¶æ¨é€åˆ°äº‘ç«¯
"""

import os
import time
import json
import requests
from datetime import datetime
from typing import List, Dict, Any
import schedule
from app.services.job_providers.openclaw_browser_provider import OpenClawBrowserProvider
from app.services.job_providers.base import JobSearchParams

class OpenClawCrawlerService:
    """OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œ"""
    
    def __init__(self, cloud_api_url: str, api_key: str):
        """
        åˆå§‹åŒ–çˆ¬è™«æœåŠ¡
        
        Args:
            cloud_api_url: äº‘ç«¯APIåœ°å€ï¼Œå¦‚ https://your-app.railway.app
            api_key: APIå¯†é’¥ï¼Œç”¨äºè®¤è¯
        """
        self.cloud_api_url = cloud_api_url.rstrip('/')
        self.api_key = api_key
        self.openclaw = OpenClawBrowserProvider()
        
        # é¢„å®šä¹‰çš„çƒ­é—¨æœç´¢å…³é”®è¯
        self.hot_keywords = [
            ["Python", "åç«¯å¼€å‘"],
            ["Java", "Spring Boot"],
            ["å‰ç«¯", "React", "Vue"],
            ["ç®—æ³•å·¥ç¨‹å¸ˆ", "æœºå™¨å­¦ä¹ "],
            ["æ•°æ®åˆ†æ", "SQL"],
            ["äº§å“ç»ç†"],
            ["æµ‹è¯•å·¥ç¨‹å¸ˆ", "è‡ªåŠ¨åŒ–"],
            ["è¿ç»´", "DevOps", "Kubernetes"],
        ]
        
        self.hot_cities = ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "æ­å·", "å¹¿å·", "æˆéƒ½"]
    
    def crawl_jobs(self, keywords: List[str], location: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        çˆ¬å–å²—ä½æ•°æ®
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            location: åŸå¸‚
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            å²—ä½åˆ—è¡¨
        """
        print(f"ğŸ” å¼€å§‹çˆ¬å–ï¼š{keywords} @ {location}")
        
        try:
            params = JobSearchParams(
                keywords=keywords,
                location=location,
                limit=limit
            )
            
            jobs = self.openclaw.search_jobs(params)
            print(f"âœ… çˆ¬å–æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}")
            return []
    
    def push_to_cloud(self, jobs: List[Dict[str, Any]]) -> bool:
        """
        æ¨é€å²—ä½æ•°æ®åˆ°äº‘ç«¯
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not jobs:
            return False
        
        try:
            url = f"{self.cloud_api_url}/api/crawler/upload"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "jobs": jobs,
                "timestamp": datetime.now().isoformat(),
                "source": "openclaw_local"
            }
            
            response = requests.post(url, json=data, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æ¨é€æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥ï¼š{response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")
            return False
    
    def crawl_and_push_all(self):
        """çˆ¬å–æ‰€æœ‰çƒ­é—¨å…³é”®è¯å¹¶æ¨é€"""
        print("\n" + "="*60)
        print(f"ğŸš€ å¼€å§‹å®šæ—¶çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        total_jobs = 0
        
        for keywords in self.hot_keywords:
            for city in self.hot_cities:
                # çˆ¬å–å²—ä½
                jobs = self.crawl_jobs(keywords, city, limit=10)
                
                if jobs:
                    # æ¨é€åˆ°äº‘ç«¯
                    if self.push_to_cloud(jobs):
                        total_jobs += len(jobs)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(5)
        
        print("\n" + "="*60)
        print(f"âœ… æœ¬æ¬¡ä»»åŠ¡å®Œæˆï¼šå…±çˆ¬å–å¹¶æ¨é€ {total_jobs} ä¸ªå²—ä½")
        print("="*60 + "\n")
    
    def start_scheduled_crawling(self, interval_hours: int = 6):
        """
        å¯åŠ¨å®šæ—¶çˆ¬å–
        
        Args:
            interval_hours: çˆ¬å–é—´éš”ï¼ˆå°æ—¶ï¼‰
        """
        print("\n" + "ğŸ¤–"*30)
        print("OpenClawçˆ¬è™«æœåŠ¡å¯åŠ¨")
        print("ğŸ¤–"*30)
        print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯ï¼š")
        print(f"  - äº‘ç«¯API: {self.cloud_api_url}")
        print(f"  - çˆ¬å–é—´éš”: æ¯ {interval_hours} å°æ—¶")
        print(f"  - å…³é”®è¯æ•°: {len(self.hot_keywords)}")
        print(f"  - åŸå¸‚æ•°: {len(self.hot_cities)}")
        print(f"\nâš ï¸ è¯·ç¡®ä¿ï¼š")
        print(f"  1. Chromeå·²æ‰“å¼€Bossç›´è˜å¹¶ç™»å½•")
        print(f"  2. OpenClawæ‰©å±•å·²Attachåˆ°æ ‡ç­¾é¡µ")
        print(f"  3. ä¿æŒæµè§ˆå™¨çª—å£ä¸è¦å…³é—­")
        print(f"\nğŸ”„ é¦–æ¬¡çˆ¬å–å°†åœ¨å¯åŠ¨åç«‹å³å¼€å§‹...\n")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        self.crawl_and_push_all()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_hours).hours.do(self.crawl_and_push_all)
        
        print(f"â° ä¸‹æ¬¡çˆ¬å–æ—¶é—´ï¼š{interval_hours} å°æ—¶å")
        print(f"ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    # é…ç½®
    CLOUD_API_URL = os.getenv("CLOUD_API_URL", "https://your-app.railway.app")
    API_KEY = os.getenv("CRAWLER_API_KEY", "your-secret-key")
    INTERVAL_HOURS = int(os.getenv("CRAWL_INTERVAL_HOURS", "6"))
    
    # å¯åŠ¨çˆ¬è™«æœåŠ¡
    crawler = OpenClawCrawlerService(
        cloud_api_url=CLOUD_API_URL,
        api_key=API_KEY
    )
    
    try:
        crawler.start_scheduled_crawling(interval_hours=INTERVAL_HOURS)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ çˆ¬è™«æœåŠ¡å·²åœæ­¢")

OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œï¼Œå®šæ—¶çˆ¬å–Bossç›´è˜å²—ä½å¹¶æ¨é€åˆ°äº‘ç«¯
"""

import os
import time
import json
import requests
from datetime import datetime
from typing import List, Dict, Any
import schedule
from app.services.job_providers.openclaw_browser_provider import OpenClawBrowserProvider
from app.services.job_providers.base import JobSearchParams

class OpenClawCrawlerService:
    """OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œ"""
    
    def __init__(self, cloud_api_url: str, api_key: str):
        """
        åˆå§‹åŒ–çˆ¬è™«æœåŠ¡
        
        Args:
            cloud_api_url: äº‘ç«¯APIåœ°å€ï¼Œå¦‚ https://your-app.railway.app
            api_key: APIå¯†é’¥ï¼Œç”¨äºè®¤è¯
        """
        self.cloud_api_url = cloud_api_url.rstrip('/')
        self.api_key = api_key
        self.openclaw = OpenClawBrowserProvider()
        
        # é¢„å®šä¹‰çš„çƒ­é—¨æœç´¢å…³é”®è¯
        self.hot_keywords = [
            ["Python", "åç«¯å¼€å‘"],
            ["Java", "Spring Boot"],
            ["å‰ç«¯", "React", "Vue"],
            ["ç®—æ³•å·¥ç¨‹å¸ˆ", "æœºå™¨å­¦ä¹ "],
            ["æ•°æ®åˆ†æ", "SQL"],
            ["äº§å“ç»ç†"],
            ["æµ‹è¯•å·¥ç¨‹å¸ˆ", "è‡ªåŠ¨åŒ–"],
            ["è¿ç»´", "DevOps", "Kubernetes"],
        ]
        
        self.hot_cities = ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "æ­å·", "å¹¿å·", "æˆéƒ½"]
    
    def crawl_jobs(self, keywords: List[str], location: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        çˆ¬å–å²—ä½æ•°æ®
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            location: åŸå¸‚
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            å²—ä½åˆ—è¡¨
        """
        print(f"ğŸ” å¼€å§‹çˆ¬å–ï¼š{keywords} @ {location}")
        
        try:
            params = JobSearchParams(
                keywords=keywords,
                location=location,
                limit=limit
            )
            
            jobs = self.openclaw.search_jobs(params)
            print(f"âœ… çˆ¬å–æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}")
            return []
    
    def push_to_cloud(self, jobs: List[Dict[str, Any]]) -> bool:
        """
        æ¨é€å²—ä½æ•°æ®åˆ°äº‘ç«¯
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not jobs:
            return False
        
        try:
            url = f"{self.cloud_api_url}/api/crawler/upload"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "jobs": jobs,
                "timestamp": datetime.now().isoformat(),
                "source": "openclaw_local"
            }
            
            response = requests.post(url, json=data, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æ¨é€æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥ï¼š{response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")
            return False
    
    def crawl_and_push_all(self):
        """çˆ¬å–æ‰€æœ‰çƒ­é—¨å…³é”®è¯å¹¶æ¨é€"""
        print("\n" + "="*60)
        print(f"ğŸš€ å¼€å§‹å®šæ—¶çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        total_jobs = 0
        
        for keywords in self.hot_keywords:
            for city in self.hot_cities:
                # çˆ¬å–å²—ä½
                jobs = self.crawl_jobs(keywords, city, limit=10)
                
                if jobs:
                    # æ¨é€åˆ°äº‘ç«¯
                    if self.push_to_cloud(jobs):
                        total_jobs += len(jobs)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(5)
        
        print("\n" + "="*60)
        print(f"âœ… æœ¬æ¬¡ä»»åŠ¡å®Œæˆï¼šå…±çˆ¬å–å¹¶æ¨é€ {total_jobs} ä¸ªå²—ä½")
        print("="*60 + "\n")
    
    def start_scheduled_crawling(self, interval_hours: int = 6):
        """
        å¯åŠ¨å®šæ—¶çˆ¬å–
        
        Args:
            interval_hours: çˆ¬å–é—´éš”ï¼ˆå°æ—¶ï¼‰
        """
        print("\n" + "ğŸ¤–"*30)
        print("OpenClawçˆ¬è™«æœåŠ¡å¯åŠ¨")
        print("ğŸ¤–"*30)
        print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯ï¼š")
        print(f"  - äº‘ç«¯API: {self.cloud_api_url}")
        print(f"  - çˆ¬å–é—´éš”: æ¯ {interval_hours} å°æ—¶")
        print(f"  - å…³é”®è¯æ•°: {len(self.hot_keywords)}")
        print(f"  - åŸå¸‚æ•°: {len(self.hot_cities)}")
        print(f"\nâš ï¸ è¯·ç¡®ä¿ï¼š")
        print(f"  1. Chromeå·²æ‰“å¼€Bossç›´è˜å¹¶ç™»å½•")
        print(f"  2. OpenClawæ‰©å±•å·²Attachåˆ°æ ‡ç­¾é¡µ")
        print(f"  3. ä¿æŒæµè§ˆå™¨çª—å£ä¸è¦å…³é—­")
        print(f"\nğŸ”„ é¦–æ¬¡çˆ¬å–å°†åœ¨å¯åŠ¨åç«‹å³å¼€å§‹...\n")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        self.crawl_and_push_all()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_hours).hours.do(self.crawl_and_push_all)
        
        print(f"â° ä¸‹æ¬¡çˆ¬å–æ—¶é—´ï¼š{interval_hours} å°æ—¶å")
        print(f"ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    # é…ç½®
    CLOUD_API_URL = os.getenv("CLOUD_API_URL", "https://your-app.railway.app")
    API_KEY = os.getenv("CRAWLER_API_KEY", "your-secret-key")
    INTERVAL_HOURS = int(os.getenv("CRAWL_INTERVAL_HOURS", "6"))
    
    # å¯åŠ¨çˆ¬è™«æœåŠ¡
    crawler = OpenClawCrawlerService(
        cloud_api_url=CLOUD_API_URL,
        api_key=API_KEY
    )
    
    try:
        crawler.start_scheduled_crawling(interval_hours=INTERVAL_HOURS)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ çˆ¬è™«æœåŠ¡å·²åœæ­¢")

OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œï¼Œå®šæ—¶çˆ¬å–Bossç›´è˜å²—ä½å¹¶æ¨é€åˆ°äº‘ç«¯
"""

import os
import time
import json
import requests
from datetime import datetime
from typing import List, Dict, Any
import schedule
from app.services.job_providers.openclaw_browser_provider import OpenClawBrowserProvider
from app.services.job_providers.base import JobSearchParams

class OpenClawCrawlerService:
    """OpenClawçˆ¬è™«æœåŠ¡ - æœ¬åœ°è¿è¡Œ"""
    
    def __init__(self, cloud_api_url: str, api_key: str):
        """
        åˆå§‹åŒ–çˆ¬è™«æœåŠ¡
        
        Args:
            cloud_api_url: äº‘ç«¯APIåœ°å€ï¼Œå¦‚ https://your-app.railway.app
            api_key: APIå¯†é’¥ï¼Œç”¨äºè®¤è¯
        """
        self.cloud_api_url = cloud_api_url.rstrip('/')
        self.api_key = api_key
        self.openclaw = OpenClawBrowserProvider()
        
        # é¢„å®šä¹‰çš„çƒ­é—¨æœç´¢å…³é”®è¯
        self.hot_keywords = [
            ["Python", "åç«¯å¼€å‘"],
            ["Java", "Spring Boot"],
            ["å‰ç«¯", "React", "Vue"],
            ["ç®—æ³•å·¥ç¨‹å¸ˆ", "æœºå™¨å­¦ä¹ "],
            ["æ•°æ®åˆ†æ", "SQL"],
            ["äº§å“ç»ç†"],
            ["æµ‹è¯•å·¥ç¨‹å¸ˆ", "è‡ªåŠ¨åŒ–"],
            ["è¿ç»´", "DevOps", "Kubernetes"],
        ]
        
        self.hot_cities = ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "æ­å·", "å¹¿å·", "æˆéƒ½"]
    
    def crawl_jobs(self, keywords: List[str], location: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        çˆ¬å–å²—ä½æ•°æ®
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            location: åŸå¸‚
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            å²—ä½åˆ—è¡¨
        """
        print(f"ğŸ” å¼€å§‹çˆ¬å–ï¼š{keywords} @ {location}")
        
        try:
            params = JobSearchParams(
                keywords=keywords,
                location=location,
                limit=limit
            )
            
            jobs = self.openclaw.search_jobs(params)
            print(f"âœ… çˆ¬å–æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{str(e)}")
            return []
    
    def push_to_cloud(self, jobs: List[Dict[str, Any]]) -> bool:
        """
        æ¨é€å²—ä½æ•°æ®åˆ°äº‘ç«¯
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not jobs:
            return False
        
        try:
            url = f"{self.cloud_api_url}/api/crawler/upload"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "jobs": jobs,
                "timestamp": datetime.now().isoformat(),
                "source": "openclaw_local"
            }
            
            response = requests.post(url, json=data, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æ¨é€æˆåŠŸï¼š{len(jobs)} ä¸ªå²—ä½")
                return True
            else:
                print(f"âŒ æ¨é€å¤±è´¥ï¼š{response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)}")
            return False
    
    def crawl_and_push_all(self):
        """çˆ¬å–æ‰€æœ‰çƒ­é—¨å…³é”®è¯å¹¶æ¨é€"""
        print("\n" + "="*60)
        print(f"ğŸš€ å¼€å§‹å®šæ—¶çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        total_jobs = 0
        
        for keywords in self.hot_keywords:
            for city in self.hot_cities:
                # çˆ¬å–å²—ä½
                jobs = self.crawl_jobs(keywords, city, limit=10)
                
                if jobs:
                    # æ¨é€åˆ°äº‘ç«¯
                    if self.push_to_cloud(jobs):
                        total_jobs += len(jobs)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(5)
        
        print("\n" + "="*60)
        print(f"âœ… æœ¬æ¬¡ä»»åŠ¡å®Œæˆï¼šå…±çˆ¬å–å¹¶æ¨é€ {total_jobs} ä¸ªå²—ä½")
        print("="*60 + "\n")
    
    def start_scheduled_crawling(self, interval_hours: int = 6):
        """
        å¯åŠ¨å®šæ—¶çˆ¬å–
        
        Args:
            interval_hours: çˆ¬å–é—´éš”ï¼ˆå°æ—¶ï¼‰
        """
        print("\n" + "ğŸ¤–"*30)
        print("OpenClawçˆ¬è™«æœåŠ¡å¯åŠ¨")
        print("ğŸ¤–"*30)
        print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯ï¼š")
        print(f"  - äº‘ç«¯API: {self.cloud_api_url}")
        print(f"  - çˆ¬å–é—´éš”: æ¯ {interval_hours} å°æ—¶")
        print(f"  - å…³é”®è¯æ•°: {len(self.hot_keywords)}")
        print(f"  - åŸå¸‚æ•°: {len(self.hot_cities)}")
        print(f"\nâš ï¸ è¯·ç¡®ä¿ï¼š")
        print(f"  1. Chromeå·²æ‰“å¼€Bossç›´è˜å¹¶ç™»å½•")
        print(f"  2. OpenClawæ‰©å±•å·²Attachåˆ°æ ‡ç­¾é¡µ")
        print(f"  3. ä¿æŒæµè§ˆå™¨çª—å£ä¸è¦å…³é—­")
        print(f"\nğŸ”„ é¦–æ¬¡çˆ¬å–å°†åœ¨å¯åŠ¨åç«‹å³å¼€å§‹...\n")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        self.crawl_and_push_all()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_hours).hours.do(self.crawl_and_push_all)
        
        print(f"â° ä¸‹æ¬¡çˆ¬å–æ—¶é—´ï¼š{interval_hours} å°æ—¶å")
        print(f"ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    # é…ç½®
    CLOUD_API_URL = os.getenv("CLOUD_API_URL", "https://your-app.railway.app")
    API_KEY = os.getenv("CRAWLER_API_KEY", "your-secret-key")
    INTERVAL_HOURS = int(os.getenv("CRAWL_INTERVAL_HOURS", "6"))
    
    # å¯åŠ¨çˆ¬è™«æœåŠ¡
    crawler = OpenClawCrawlerService(
        cloud_api_url=CLOUD_API_URL,
        api_key=API_KEY
    )
    
    try:
        crawler.start_scheduled_crawling(interval_hours=INTERVAL_HOURS)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ çˆ¬è™«æœåŠ¡å·²åœæ­¢")



